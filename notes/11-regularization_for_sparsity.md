> 学习目标：
>
> - 了解如何使信息缺乏的系数值正好为 0，以便节省 RAM
> - 了解 L2 正则化之外的其他类型的正则化

# 第11节 稀疏性正则化(L1正则化) 

稀疏矢量往往包含了许多维度，创建特征组合会导致包含更多的维度，这样可能会造成模型十分庞大，且需要大量的RAM。

在高维度稀疏矢量中，最好尽可能使权重降到0，因为权重0连接的特征会从模型中移除。特征设置为0可以节省RAM空间，而且可以减少模型中的噪点。

> **例子：**以一个涵盖全球地区（不仅仅只是涵盖加利福尼亚州）的住房数据集为例。如果按分（每度为 60 分）对全球纬度进行分桶，则在一次稀疏编码过程中会产生大约 1 万个维度；如果按分对全球经度进行分桶，则在一次稀疏编码过程中会产生大约 2 万个维度。这两种特征的特征组合会产生大约 2 亿个维度。这 2 亿个维度中的很多维度代表非常有限的居住区域（例如海洋里），很难使用这些数据进行有效泛化。 若为这些不需要的维度支付 RAM 存储费用就太不明智了。 因此，最好是使无意义维度的权重正好降至 0，这样我们就可以避免在推理时支付这些模型系数的存储费用。



我们可以添加适当的正则化项，将这种想法变成在训练期间解决的优化问题。

- $L_2$正则化可以使权重变小，但是不能使它们恰好为0.0
- 我们设想创建一个正则化项，减少模型中非零系数值的计数。只有在模型能够与数据拟合时增加此计数才有意义。虽然这种计数方法看起来很有吸引力，但是它会把我们的凸优化问题变为非凸优化问题(NP-hard)。所有，$L_0$正则化在实践中并不是一种有效的方法。
- $L_1$正则化作用类似于$L_0$，但是它具有凸优化的优势，可以有效的进行计算。因此，我们可以使用$L_1$正则化使模型中很多信息缺乏的系数正好为0，从而在推理时节省RAM。

## 11.1. $L_1$和$L_2$正则化

L2 和 L1 采用不同的方式降低权重：

- L2会降低(权重^2)
- L1会降低(|权重|)

因此，两者具有不同的导数:

- L2的导数为(2 * 权重)
- L1的导数为(k)，这是一个与权重无关的常数

我们可以将 L2 的导数的作用理解为每次移除权重的 x%。对于任意数字，即使按每次减去 x% 的幅度执行数十亿次减法计算，最后得出的值也绝不会正好为 0(除非浮点精度限制)。L2 通常不会使权重变为 0。

我们可以将 L1 的导数的作用理解为每次从权重中减去一个常数。不过，由于减去的是绝对值，L1在 0 处具有不连续性，这会导致与 0 相交的减法结果变为 0。例如，如果减法使权重从 +0.1 变为 -0.2，L1 便会将权重设为 0。

>  L1正则化，减少所有权重的绝对值，证明对宽度模型十分有效。这条说明对一维模型有效。

编程参见[l1_regularization.ipynb](../code/l1_regularization.ipynb)

